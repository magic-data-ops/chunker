You are analyzing a corpus of California state case law to generate evaluation
prompts for an LLM. Each evaluation prompt tests a single capability, denoted
by its "category."

DOMAIN CONSTRAINT: All questions, answers, and evidence MUST come exclusively
from California state case law found in this corpus. Do not reference federal
cases, other states' cases, or non-legal content.

You are operating inside the corpus directory. Available files:
{{FILE_LIST}}

---

CATEGORY: {{CATEGORY_NAME}}
CAPABILITY BEING EVALUATED: {{CATEGORY_DESCRIPTION}}

---

SEED CONTEXT (starting point — use this to orient your search):
{{SEED_CONTEXT}}

---

Your task: look at the source material and find relevant content to create exactly
{{N_PAIRS}} evaluation prompt(s) that test the "{{CATEGORY_NAME}}" capability.
For each prompt, you must also provide the golden response (the ideal answer an LLM
should produce).

SEARCH STRATEGY:

1. Start from the seed context above. Identify key multi-word phrases, names, or concepts.
2. Use Glob to discover available .txt files if needed.
3. Use Grep with specific multi-word phrases (2–4 words) to find relevant passages. Examples:
   - Grep for "Brown v. Board" — good (specific case name)
   - Grep for "court" — bad (too generic, overwhelming results)
   - Grep for "Penal Code Section 1119" — good (specific statute)
   - Grep for "section" — bad (too broad)
4. Use Read to gather full context around matches. Read enough surrounding lines
   (typically 30–100 lines around a hit) to fully understand the passage.
5. Category-specific search guidance:
   - For cross_context_synthesis: Find multiple passages on the same legal topic from
     different cases that must be combined for a complete picture.
   - For long_context_citation: Find a specific fact and verify the exact case name,
     volume, and page number from the corpus text.
   - For hierarchy_comprehension: Find passages that show structural relationships —
     majority vs dissent, headnotes vs holdings, procedural vs substantive.
   - For entity_state_tracking: Find a party, statute, or legal standard that changes
     across multiple proceedings or cases.
   - For entity_disambiguation: Search for a name or term that appears in two or more
     clearly different contexts (different cases, different parties, different legal
     concepts). The more distinct the contexts, the better.
   - For multi_hop_reasoning: Search for a concept from the seed, then search for a
     related concept from what you found. Chain at least 2 separate passages from
     different parts of the corpus.
   - For semantic_deduplication: Find two passages from different cases stating the same
     legal principle in different words.
   - For temporal_ordering: Find events (filings, rulings, amendments) that appear out
     of chronological order in the corpus.
   - For domain_scoping: Find a legal principle and verify its jurisdictional or
     subject-matter boundaries (civil vs criminal, state vs federal, etc.).
   - For source_prioritization: Find conflicting holdings where one supersedes another
     by recency, court authority, or specificity.
   - For numerical_aggregation: Find multiple numerical facts (damages, counts, dates)
     that require arithmetic to combine.
   - For conflicting_information_synthesis: Find cases with opposing holdings, dissents,
     or contradictory legal reasoning on the same issue.

PARALLEL SEARCH WITH SUBAGENTS — STRONGLY RECOMMENDED:
You have access to the Task tool which spawns Explore subagents. These subagents run
in parallel, each with their own context window, allowing you to search far more of
the corpus than you could alone. USE THEM AGGRESSIVELY.

When to spawn subagents (do this by default for most searches):
- Spawn 2–4 Explore agents, each searching for different aspects of the category.
- Each agent can use Grep, Read, and Glob independently.
- Launch multiple agents simultaneously for maximum parallelism.

Example — entity disambiguation:
  Task(subagent_type="Explore", prompt="Search the corpus for California cases
  involving a party named 'Smith' in a property dispute. Use Grep for 'Smith' near
  property-related terms. Report all passages with file names and line numbers.")

  Task(subagent_type="Explore", prompt="Search the corpus for California cases
  involving a party named 'Smith' in a criminal matter. Use Grep for 'Smith' near
  criminal terms. Report all passages with file names and line numbers.")

Example — temporal ordering:
  Task(subagent_type="Explore", prompt="Find all passages mentioning dates, filings,
  or procedural events related to property law. Report each with exact dates and
  line numbers.")

The subagents extend your effective context window — a single model can only read so
much, but 3–4 parallel subagents can collectively survey the entire corpus. Always
prefer spawning subagents over sequential Grep/Read when the corpus is large.

EVIDENCE QUALITY — CRITICAL:

Your evidence_snippets MUST be verbatim excerpts of COMPLETE SENTENCES from the corpus.

GOOD evidence (full sentences, ≥50 characters):
  "The court held that a defendant's right to oral witness examination could not be frittered away by compelling him to go to trial without the benefit of their testimony."
  "This suit was brought to procure the cancellation of three warrants for the sum of fifteen hundred dollars each."

BAD evidence (single words, fragments — these will be REJECTED):
  "the court"
  "property"
  "1856"
  "the defendant"

Each evidence snippet must be:
- A verbatim copy-paste from the corpus (no paraphrasing whatsoever)
- At least 50 characters long
- A complete sentence or meaningful clause
- Directly supporting the answer
- From California state case law

PROCESS:

Phase 1 — Explore the Source:
1. Use Grep to search the corpus for content relevant to the "{{CATEGORY_NAME}}" capability.
2. Use Read to gather sufficient surrounding context (read enough lines to fully
   understand each passage — case names, parties, holdings, key details).
3. Identify {{N_PAIRS}} distinct, well-separated examples from different parts of the corpus.

Phase 2 — Construct Evaluation Prompts and Golden Responses:
For each example, craft:

- QUESTION (the evaluation prompt): A detailed question that requires genuine capability
  in "{{CATEGORY_NAME}}" to answer correctly. The question must be answerable solely from
  the corpus and must be non-trivial. Must end with a question mark.
  Be specific about what information to provide (e.g., case names, holdings, statutory
  references, party names, dates).

- GOLDEN_ANSWER (the ideal response): A comprehensive, structured answer that a
  top-performing LLM should produce. This is the ground-truth answer against which LLM
  responses will be scored — it must be thorough and precise. Include:
  * All relevant case names, citations (volume Cal. page), and years
  * Precise legal holdings using language from the source
  * Clear structure with labeled sections where appropriate
  * All relevant facts, names, dates, and identifiers

- DIFFICULTY: "easy", "medium", or "hard" based on how many passages must be found,
  how subtle the reasoning is, and how much legal domain knowledge is required.

- ENTITIES: An array of the key entities found, each with:
  * label: a short identifier (e.g., "People v. Diaz — Confrontation Right")
  * description: what this entity is, using source language
  * evidence_snippet: verbatim excerpt (≥50 chars) supporting this entity
  * evidence_location: file and line range where this entity's evidence was found

- DISAMBIGUATION_STATEMENT: A clear explanation of how the entities or passages relate,
  differ, or connect — appropriate to the category being tested.

- SOURCE_FILES: filename(s) where evidence was found.

QUALITY RULES:
- Each question must be distinct — do not repeat the same case, entity, or concept.
- The golden answer must be fully supported by the corpus (no hallucination).
  Every claim must trace back to a verbatim evidence snippet.
- All content must come from California state case law in the corpus.
- Evidence snippets must include enough context to verify the claim (case name, holding).

OUTPUT FORMAT:
Respond with ONLY a valid JSON array. No preamble, no explanation, no markdown code fences.

[
  {
    "question": "<detailed evaluation prompt ending with ?>",
    "golden_answer": "<comprehensive ideal response>",
    "difficulty": "<easy|medium|hard>",
    "entities": [
      {
        "label": "<short entity identifier>",
        "description": "<what this entity is, from the source>",
        "evidence_snippet": "<verbatim sentence from corpus, ≥50 chars>",
        "evidence_location": {"file": "<filename>", "start_line": 100, "end_line": 150}
      }
    ],
    "disambiguation_statement": "<how the entities/passages relate or differ>",
    "source_files": ["<filename>"],
    "evidence_snippets": ["<verbatim sentence ≥50 chars>", "<another verbatim sentence>"],
    "evidence_locations": [
      {"file": "<filename>", "start_line": 100, "end_line": 150},
      {"file": "<filename>", "start_line": 2000, "end_line": 2050}
    ]
  }
]
